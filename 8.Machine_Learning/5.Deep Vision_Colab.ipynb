{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes convolucionales\n",
    "\n",
    "## Pooling\n",
    "\n",
    "En el ejemplo de convnet, habrás notado que el tamaño de los mapas de características se reduce a la mitad después de cada capa `MaxPooling2D`. Por ejemplo, antes de las primeras capas `MaxPooling2D`, el mapa de características es de `26 × 26`, pero la operación de agrupación máxima lo reduce a la mitad a `13 × 13`. Esa es la función de `maxpooling`: reducir agresivamente la resolución de los mapas de características, de forma muy similar a las convoluciones a `strides`.\n",
    "\n",
    "El `maxpooling` consiste en extraer ventanas de los mapas de características de entrada y generar el valor máximo de cada canal. Es conceptualmente similar a la convolución, excepto que en lugar de transformar parches locales mediante una transformación lineal aprendida (el núcleo de convolución), se transforman mediante una operación de tensor máximo codificada. Una gran diferencia con la convolución es que la agrupación máxima generalmente se realiza con ventanas de `2 × 2` y `stride` `2`, para reducir la muestra de los mapas de características en un factor de `2`. Por otro lado, la convolución generalmente se realiza con ventanas de `3 × 3` y sin `stride` (zancada 1).\n",
    "\n",
    "- Se usan para hacer **downsampling del input feature** (reducir dimensionalidad espacial)\n",
    "- Similar a convoluciones, pero no aplican un kernel sino que aplican **funcion 'max' (o average)**\n",
    "- `Se fijan en el vecindario y se quedan con el promedio o con el máximo.`\n",
    "- `Por defecto, tamaño 2x2 y stride 2` Si tengo un mapa de 32 x 32 me lo pasa a uno de 16 x 16\n",
    "- `Las operaciones Pooling siempre se ponene detras de convolucional`\n",
    "- Esto nos permite reducir el coste computacional\n",
    "- Se puede hacer con Stride, pero es más común realizarlo por Pooling\n",
    "- Max Pooling tienen más popularidad que el AVG pooling, ya que si dejo pasar los valores máximos, los termino perdiendo, se difimunan.\n",
    "\n",
    "\n",
    "#### ¿Para qué?\n",
    "\n",
    "-   Reducir el número de parámetros\n",
    "-   Permite a ventanas posteriores aceptar input de areas más grandes en capas anteriores (mejora percepción)\n",
    "\n",
    "#### En resumen\n",
    "\n",
    "La razón para utilizar `maxpooling` es reducir el número de coeficientes del mapa de características a procesar, así como inducir jerarquías de filtros espaciales al hacer que las capas de convolución sucesivas miren ventanas cada vez más grandes (en términos de la fracción de la entrada original). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Based Model: Encargado de extraer las características de las imágenes.\n",
    "\"\"\"\n",
    "#BASE MODEL (Destinada a FE)\n",
    "\n",
    "\"\"\"\n",
    "32,(3,3) \n",
    "- 32 es el número de filtros que va a contener esa capa\n",
    "- (3,3) es el tamaño del filtro\n",
    "- input_shape=(32,32,3)\n",
    "\"\"\"\n",
    "\n",
    "convnet_nopooling = Sequential()\n",
    "convnet_nopooling.add(layers.Conv2D(32,(3,3),input_shape=(32,32,3),activation='relu'))\n",
    "convnet_nopooling.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "convnet_nopooling.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "convnet_nopooling.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "\n",
    "\"\"\"\n",
    "Top Model: Aplano características\n",
    "A este Perceptron multicapa ya le entran las caracteríticas aplanadas del Base Model.\n",
    "\"\"\"\n",
    "\n",
    "#TOP MODEL\n",
    "\n",
    "\"\"\"\n",
    "Estoy aplanando las caracteristicas.\n",
    "\"\"\"\n",
    "convnet_nopooling.add(layers.Flatten())\n",
    "convnet_nopooling.add(layers.Dense(512,activation='relu'))\n",
    "convnet_nopooling.add(layers.Dense(10,activation='softmax'))\n",
    "\n",
    "convnet_nopooling.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convnet_pooling = Sequential()\n",
    "\n",
    "########################## BASE MODEL ###### ENCARGADO DE EXTRAER CARACTERÍSTICAS AUTOMÁTICAMENTE\n",
    "\n",
    "\"\"\"\n",
    "Debo de identificar el problema para poder implementar el bloque que corrresponda para realidad pooling\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#BLOQUE CONVOLUCIONAL 1\n",
    "convnet_pooling.add(layers.Conv2D(32,(3,3),input_shape=(32,32,3),activation='relu'))\n",
    "convnet_pooling.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "convnet_pooling.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "#BLOQUE CONVOLUCIONAL 2\n",
    "convnet_pooling.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "convnet_pooling.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "convnet_pooling.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "########################## TOP MODEL ###### ENCARGADO LLEVAR A CABO LA ######CLASIFICACIÓN\n",
    "convnet_pooling.add(layers.Flatten())\n",
    "convnet_pooling.add(layers.Dense(512,activation='relu'))\n",
    "convnet_pooling.add(layers.Dense(10,activation='softmax'))\n",
    "convnet_pooling.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este modelo, cambiamos la cantidad de parámetros que maneja.\n",
    "Podemos observar que luego de la capa convolucional `conv2d_6`, al entrar el `max_pooling`, me baja el shape de `28,28,32` a `14,14,32`\n",
    "\n",
    "Luego de la `conv2d_8`, al aplicar la `max_pooling2d_2`, vuelvo a reducir de `10,10,64` a `5,5,64` Como resultado me da 1600 features en mi capa aplanda comparado contra los cientos de miles del modelo sin `maxpooling`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parches (Patch)\n",
    "\n",
    "**Tamaño de los parches** (habitualmente 3x3 o 5x5)\n",
    "-   Depth (profundidad, **número de filtros**) del mapa de features del output (32 y 64 en el ejemplo)\n",
    "\n",
    "`Conv2D(output_depth,(patch_height,patch_width))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendiendo los efectos de borde y el relleno (Padding)\n",
    "\n",
    "Considere un mapa de características de `5 × 5` (25 mosaicos en total). Sólo hay `9` mosaicos alrededor de los cuales puedes centrar una ventana de `3 × 3`, formando una cuadrícula de 3 × 3. Por lo tanto, el mapa de características de salida será de `3 × 3`. Se reduce un poco: en este caso, exactamente dos mosaicos a lo largo de cada dimensión. Puedes ver este efecto de borde en acción en el ejemplo anterior: comienzas con entradas de `28 × 28`, que se convierten en `26 × 26` después de la primera capa de convolución.\n",
    "\n",
    "![](./img/patch_1.png)\n",
    "\n",
    "Si desea obtener un mapa de características de salida con las mismas dimensiones espaciales que la entrada, puede usar relleno. El relleno consiste en agregar una cantidad adecuada de filas y columnas a cada lado del mapa de características de entrada para que sea posible ajustar ventanas de convolución centrales alrededor de cada mosaico de entrada. Para una ventana de `3 × 3`, agrega una columna a la derecha, una columna a la izquierda, una fila en la parte superior y una fila en la parte inferior. Para una ventana de `5 × 5`, agrega dos filas\n",
    "\n",
    "![](./img/patch_2.png)\n",
    "\n",
    "En las capas de Conv2D, el relleno se puede configurar mediante el argumento padding, que toma dos valores: `valid`, que significa que no hay relleno (solo se usarán ubicaciones de ventana válidas) y `same`, que significa \"relleno de tal manera que tener una salida con el mismo ancho y alto que la entrada”. El argumento de relleno por defecto es `valid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendiendo los avances de convolución (Strides)\n",
    "\n",
    "El otro factor que puede influir en el tamaño de la producción es la noción de avances. Nuestra descripción de la convolución hasta ahora ha asumido que los mosaicos centrales de las ventanas de convolución son todos contiguos. Pero la distancia entre dos ventanas sucesivas es un parámetro de la convolución, llamado `stride`, que por defecto es `1`. \n",
    "\n",
    "Es posible tener convoluciones con `strides`: convoluciones con una `strides` superior a `1`. \n",
    "En la figura, puedes ver los parches extraídos por un Convolución `3 × 3` con `strides` `2` sobre una entrada de `5 × 5` (sin relleno).\n",
    "\n",
    "Usar paso 2 significa que el `ancho` y el `alto` del mapa de características se reducen en un factor de `2` (además de cualquier cambio inducido por los efectos de borde). Las convoluciones `strides` rara vez se utilizan en modelos de clasificación, pero resultan útiles para algunos tipos de modelos.\n",
    "\n",
    "![](./img/strides.png)\n",
    "\n",
    "En los modelos de clasificación, en lugar de avances, tendemos a utilizar la operación de agrupación máxima (`maxpooling`) para reducir la muestra de mapas de características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bKNJiApNHlj"
   },
   "source": [
    "## TRABAJANDO CON REDES PRE-ENTRENADAS: TRANSFER LEARNING & FINE-TUNING\n",
    "\n",
    "Un enfoque común y muy eficaz para el aprendizaje profundo en conjuntos de datos de imágenes pequeños es utilizar un modelo previamente entrenado (`pretrained models`). Un modelo previamente entrenado es un modelo que se entrenó previamente en un gran conjunto de datos, generalmente en una tarea de clasificación de imágenes a gran escala. \n",
    "\n",
    "Si este conjunto de datos original es lo suficientemente grande y general, la jerarquía espacial de las características aprendidas por el modelo previamente entrenado puede actuar efectivamente como un modelo genérico del mundo visual y, por lo tanto, sus características pueden resultar útiles para muchos problemas diferentes de visión por computadora, aunque Estos nuevos problemas pueden involucrar clases completamente diferentes a las de la tarea original. \n",
    "\n",
    "Por ejemplo, podría entrenar un modelo en `ImageNet` (donde las clases son principalmente animales y objetos cotidianos) y luego reutilizar este modelo entrenado para algo tan remoto como identificar muebles en imágenes. Esta portabilidad de las características aprendidas entre diferentes problemas es una ventaja clave del aprendizaje profundo en comparación con muchos enfoques de aprendizaje superficial más antiguos, y hace que el aprendizaje profundo sea muy eficaz para problemas de datos pequeños.\n",
    "\n",
    "Usaremos la arquitectura `VGG16`, desarrollada por `Karen Simonyan` y `Andrew Zisserman` en 2014. Aunque es un modelo antiguo, alejado del estado actual del arte y algo más pesado que muchos otros modelos recientes, su arquitectura es similar a la que ya está familiarizado y es fácil de entender sin introducir ningún concepto nuevo. \n",
    "\n",
    "Este puede ser su primer encuentro con uno de estos nombres de modelos: `VGG`, `ResNet`, `Inception`, `Xception`, etc.\n",
    "\n",
    "### Extracción de características con un modelo previamente entrenado\n",
    "\n",
    "La extracción de características consiste en utilizar las representaciones aprendidas por un modelo previamente entrenado para extraer características interesantes de nuevas muestras. Luego, estas características se ejecutan a través de un nuevo clasificador, que se entrena desde cero.\n",
    "\n",
    "Como vio anteriormente, las convnets utilizadas para la clasificación de imágenes constan de dos partes: comienzan con una serie de capas de agrupación y convolución, y terminan con un clasificador densamente conectado. La primera parte se llama base convolucional del modelo. En el caso de las convnets, la extracción de características consiste en tomar la base convolucional de una red previamente entrenada, ejecutar los nuevos datos a través de ella y entrenar un nuevo clasificador encima de la salida.\n",
    "\n",
    "![](./img/pretrained_1.png)\n",
    "\n",
    "\n",
    "\n",
    "¿Por qué reutilizar únicamente la base convolucional? ¿Podríamos reutilizar también el clasificador densamente conectado? En general, se debe evitar hacerlo. La razón es que las representaciones aprendidas por la base convolucional probablemente sean más genéricas y, por lo tanto, más reutilizables: los mapas de características de una convnet son mapas de presencia de conceptos genéricos sobre una imagen, que probablemente sean útiles independientemente de la computadora. \n",
    "\n",
    "Pero las representaciones aprendidas por el clasificador serán necesariamente específicas del conjunto de clases en las que se entrenó el modelo; solo contendrán información sobre la probabilidad de presencia de tal o cual clase en la imagen completa. Además, las representaciones que se encuentran en capas densamente conectadas ya no contienen información sobre dónde se encuentran los objetos en la imagen de entrada; Estas capas eliminan la noción de espacio, mientras que la ubicación del objeto todavía se describe mediante mapas de características convolucionales. Para problemas en los que la ubicación de los objetos importa, las funciones densamente conectadas son en gran medida inútiles.\n",
    "\n",
    "Tenga en cuenta que el nivel de generalidad (y por lo tanto de reutilización) de las representaciones extraídas por capas convolucionales específicas depende de la profundidad de la capa en el modelo. Las capas que aparecen antes en el modelo extraen mapas de características locales y altamente genéricos (como bordes visuales, colores y texturas), mientras que las capas que están más arriba extraen conceptos más abstractos (como “oreja de gato” u “ojo de perro”). \n",
    "\n",
    "Entonces, si tu nuevo conjunto de datos difiere mucho del conjunto de datos en el que se entrenó el modelo original, es mejor que utilices solo las primeras capas del modelo para realizar la extracción de características, en lugar de utilizar toda la base convolucional.\n",
    "\n",
    "\n",
    "El modelo VGG16, entre otros, viene preempaquetado con `Keras`. Puedes importarlo desde el módulo `keras.applications`. Muchos otros modelos de clasificación de imágenes (todos previamente entrenados en el conjunto de datos ImageNet) están disponibles como parte de `keras.applications`:\n",
    "\n",
    "- Xcepción\n",
    "- Resnet\n",
    "- mobile Resnet\n",
    "- Efficient net\n",
    "\n",
    "etc.\n",
    "\n",
    "![](./img/pretrained_networks.png)\n",
    "\n",
    "\n",
    "## Diferencias\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*p-2QjvJ4nDCfn3F5oIxvYA.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n64pEFv_YGu_"
   },
   "source": [
    "#### **- Cargando el conjunto de datos y acondicionándolo como en la VGG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ov7MLBjsVWjE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Importando y normalizando el set de datos CIFAR10\n",
    "print(\"[INFO]: Loading CIFAR-10 data...\")\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "labelNames = [\"Avión\", \"Automóvil\", \"Pájaro\", \"Gato\", \"Ciervo\", \"Perro\", \"Rana\", \"Caballo\", \"Barco\", \"Camión\"]\n",
    "\n",
    "#One-hot encoding\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "\n",
    "# IMPORTANTE: Se normalizan los datos como se normalizaron en el entrenamiento con ImageNet!!\n",
    "trainX = imagenet_utils.preprocess_input(trainX)\n",
    "testX = imagenet_utils.preprocess_input(testX)\n",
    "\n",
    "#print(trainX.shape)\n",
    "#print(trainY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB0W9DWJYLQo"
   },
   "source": [
    "#### **- Cargando la topología de CNN (base model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRfr9ZWeNHlm",
    "outputId": "395fe4f9-0bd3-44ed-b330-664107dfc317",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#keras incluye varias arquitecturas\n",
    "# VGG16, VGG19, ResNet50, Xception, InceptionV3, InceptionResNetV2, MobileNetV2, DenseNet, RasNet\n",
    "# documentacion https://keras.io/applications/\n",
    "# Visual Geometry Group 16 / 19 (numero de layers)\n",
    "# 1 y 2 en la competicion ImageNet 2014\n",
    "# Kernels pequeños de 3x3\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "base_model = VGG16(weights='imagenet',\n",
    "                 include_top=False, # No incluir el top model, i.e. la parte densa destinada a la clasificación (fully connected layers)\n",
    "                 input_shape=(32,32,3))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fgp6wGnDYSTs"
   },
   "source": [
    "#### **- Creando el top model y congelando TODAS las capas convolucionales (TRANSFER LEARNING)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTvDJDz2NHlq",
    "outputId": "4803e129-e952-4b98-80aa-beb720759e51",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conectarlo a nueva parte densa\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "base_model.trainable = False # Evitar que los pesos se modifiquen en la parte convolucional -> TRANSFER LEARNING\n",
    "pre_trained_model = Sequential()\n",
    "pre_trained_model.add(base_model)\n",
    "pre_trained_model.add(layers.Flatten())\n",
    "pre_trained_model.add(layers.Dense(256, activation='relu'))\n",
    "pre_trained_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaZxEUW3YYGI"
   },
   "source": [
    "#### **- Entrenando la solución**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0h_rPl2RpBz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab import drive\n",
    "\n",
    "BASE_FOLDER = './data/'\n",
    "\n",
    "# Compilar el modelo\n",
    "print(\"[INFO]: Compilando el modelo...\")\n",
    "pre_trained_model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08), metrics=[\"accuracy\"]) \n",
    "\n",
    "# Entrenamiento de la red\n",
    "print(\"[INFO]: Entrenando la red...\")\n",
    "H_pre = pre_trained_model.fit(trainX, trainY, batch_size=128, epochs=20, validation_split=0.2)\n",
    "\n",
    "# Almaceno el modelo en Drive\n",
    "# Montamos la unidad de Drive\n",
    "#drive.mount('/content/drive') \n",
    "# Almacenamos el modelo empleando la función mdoel.save de Keras\n",
    "pre_trained_model.save(BASE_FOLDER+\"deepCNN_CIFAR10_pretrained.h5\") #(X)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(\"[INFO]: Evaluando el modelo...\")\n",
    "# Efectuamos la predicción (empleamos el mismo valor de batch_size que en training)\n",
    "predictions = pre_trained_model.predict(testX, batch_size=128)\n",
    "# Sacamos el report para test\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)) \n",
    "\n",
    "# Gráficas\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 20), H_pre.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 20), H_pre.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 20), H_pre.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 20), H_pre.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDXy7tDNYpeL"
   },
   "source": [
    "## Creando el top model y descongelando bloques convolucionales (FINE TUNING)\n",
    "\n",
    "Otra técnica ampliamente utilizada para la reutilización de modelos, complementaria a la extracción de características, es el `fine tuning`. \n",
    "\n",
    "Cuando el dominio destino es poco parecido al dataset del dominio original se aplica la técnica de `Fine Tuning` a diferencia del `Transfer Learning`\n",
    "\n",
    "El `fine tuning` consiste en descongelar algunas de las capas superiores de una base de modelo congelada utilizada para la extracción de características y entrenar conjuntamente tanto la parte recién agregada del modelo (en este caso, el clasificador completamente conectado) como estas capas superiores. Esto se denomina ajuste fino porque ajusta ligeramente las representaciones más abstractas del modelo que se reutiliza para hacerlas más relevantes para el problema en cuestión.\n",
    "\n",
    "Como se ha mencionado anteriormente es necesario congelar la base de convolución de `VGG16` para poder entrenar un clasificador inicializado aleatoriamente en la parte superior. Por la misma razón, solo es posible ajustar las capas superiores de la base convolucional una vez que el clasificador superior ya ha sido entrenado. \n",
    "\n",
    "Si el clasificador aún no está entrenado, la señal de error que se propaga a través de la red durante el entrenamiento será demasiado grande y las representaciones aprendidas previamente por las capas que se están ajustando se destruirán. Por tanto, los pasos para ajustar una red son los siguientes:\n",
    "\n",
    "- Agregue nuestra red personalizada además de una red base ya capacitada.\n",
    "\n",
    "- Congele la red base.\n",
    "\n",
    "- Entrena la parte que agregamos.\n",
    "\n",
    "- Descongela algunas capas en la red base. (Tenga en cuenta que no debe descongelar las capas de “normalización por lotes”, que no son relevantes aquí ya que no existen tales capas en VGG16. La normalización por lotes y su impacto en el ajuste fino se explica en el siguiente capítulo).\n",
    "\n",
    "- Entrenar conjuntamente ambas capas y la parte que agregamos.\n",
    "\n",
    "![](./img/finetuning.png)\n",
    "\n",
    "\n",
    "Ajustaremos las últimas tres capas convolucionales, lo que significa que todas las capas hasta `block4_pool` deben congelarse, y las capas `block5_conv1`, `block5_conv2` y `block5_conv3` deben poder entrenarse.\n",
    "\n",
    "¿Por qué no ajustar más capas? ¿Por qué no ajustar toda la base convolucional? Tú podrías. Pero debes considerar lo siguiente:\n",
    "\n",
    "- Las capas anteriores en la base convolucional codifican características más genéricas y reutilizables, mientras que las capas superiores codifican características más especializadas. Es más útil ajustar las funciones más especializadas, porque son las que deben reutilizarse en su nuevo problema. Se producirían rendimientos rápidamente decrecientes si se ajustaran las capas inferiores.\n",
    "\n",
    "- Cuantos más parámetros entrenes, mayor será el riesgo de sobreajuste. La base convolucional tiene 15 millones de parámetros, por lo que sería arriesgado intentar entrenarla en su pequeño conjunto de datos.\n",
    "\n",
    "\n",
    "![](./img/finetuning_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzM0C_C9mWAy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports que vamos a necesitar\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.applications import VGG16, imagenet_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "#Cargamos el dataset CIFAR10\n",
    "(trainX, trainY), (testX, testY) = cifar10.load_data() \n",
    "\n",
    "# Normalizamos las entradas de idéntica forma a como lo hicieron para entrenar la VGG16 en imageNet\n",
    "trainX = imagenet_utils.preprocess_input(trainX) \n",
    "testX = imagenet_utils.preprocess_input(testX) \n",
    "\n",
    "# Definimos dimensiones de nuestros datos de entrada y lista con las categorias de las clases\n",
    "input_shape = (32, 32, 3) \n",
    "labelNames = [\"Avión\", \"Automóvil\", \"Pájaro\", \"Gato\", \"Ciervo\", \"Perro\", \"Rana\", \"Caballo\", \"Barco\", \"Camión\"] \n",
    "\n",
    "# En caso de inestabilidades numéricas pasar datos a one-hot encoding\n",
    "trainY = to_categorical(trainY) \n",
    "testY = to_categorical(testY) \n",
    "\n",
    "# Importamos VGG16 con pesos de imagenet y sin top_model especificando tamaño de entrada de datos\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "# Mostramos la arquitectura\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Congelamos las capas de los 4 primeros bloques convolucionales, el quinto se re-entrena\n",
    "# En base_model.layers.name tenemos la información del nombre de la capa\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "for layer in base_model.layers: \n",
    "    if layer.name == 'block3_conv1': \n",
    "        break \n",
    "    layer.trainable = False\n",
    "    print('Capa ' + layer.name + ' congelada...') \n",
    "      \n",
    "\n",
    "# Tomamos la última capa del model y le añadimos nuestro clasificador (top_model)\n",
    "last = base_model.layers[-1].output \n",
    "x = Flatten()(last) \n",
    "x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "x = Dropout(0.3)(x) \n",
    "x = Dense(256, activation='relu', name='fc2')(x) \n",
    "x = Dense(10, activation='softmax', name='predictions')(x) \n",
    "model = Model(base_model.input, x) \n",
    "\n",
    "\n",
    "# Compilamos el modelo\n",
    "model.compile(Adam(lr=0.0005), loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Vamos a visualizar el modelo prestando especial atención en el número de pesos total y el número de pesos entrenables.\n",
    "# ¿tiene sentido en comparación al ejemplo de transfer learning?\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Entrenamos el modelo\n",
    "H = model.fit(trainX, \n",
    "              trainY, \n",
    "              validation_split=0.2, \n",
    "              batch_size=64, \n",
    "              epochs=20, \n",
    "              verbose=1) \n",
    "\n",
    "# Evaluación del modelo\n",
    "print(\"[INFO]: Evaluando el modelo...\")\n",
    "predictions = model.predict(testX, batch_size=64) \n",
    "# Obtener el report de clasificación\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)) \n",
    "\n",
    "# Gráficas\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 20), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 20), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 20), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 20), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jT8F5uBbGSbh"
   },
   "source": [
    "# Summary\n",
    "\n",
    "Las `convnets` son el mejor tipo de modelos de aprendizaje automático para tareas de visión por computadora. Es posible entrenar uno desde cero incluso con un conjunto de datos muy pequeño, con resultados decentes.\n",
    "\n",
    "Los `convnets` funcionan aprendiendo una jerarquía de patrones y conceptos modulares para representar el mundo visual.\n",
    "\n",
    "En un conjunto de datos pequeño, el problema principal será el `sobreajuste`. El aumento de datos es una forma poderosa de combatir el sobreajuste cuando se trabaja con datos de imágenes.\n",
    "\n",
    "Es fácil reutilizar una `convnet` existente en un nuevo conjunto de datos mediante la extracción de funciones. Esta es una técnica valiosa para trabajar con conjuntos de datos de imágenes pequeños.\n",
    "\n",
    "Como complemento a la extracción de características, se puede utilizar el `fine tuning`, que adapta a un nuevo problema algunas de las representaciones aprendidas previamente por un modelo existente. Esto lleva el rendimiento un poco más allá."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Deep Learning with Python, Second Edition"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "oTlHf22PigwY",
    "-xgqejLMinyM",
    "0yhEXcdHi4Aj",
    "DlyJFsn_jHfj",
    "-zbIKienjdhb",
    "9QfK8aj0klct",
    "kSb1uTpJn_eC",
    "TSXhGQa3PQKc",
    "bEgHlw7oPSl6",
    "3jiAe2cEPk0Y",
    "9bKNJiApNHlj",
    "Cpe6aM9bvyyc",
    "L0nNoWlXv5J7"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
