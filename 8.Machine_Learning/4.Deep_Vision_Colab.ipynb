{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_xGbf7WNHiE"
   },
   "source": [
    "# Redes Neuronales y Deep Learning\n",
    "\n",
    "## Tipos de capas\n",
    "- Cada tipo de capa suele tener un uso (dependiente de la red a diseñar según el tipo de entrada y la aplicación):\n",
    " - **Dense** para tratar input vector o **datos estructurados** (2D tensor de samples,features)\n",
    " - **LSTM** (Recurrent Neural Networks) para tratar **secuencias lógicas** (3D tensores de samples(Eje x),timestep (Eje y),features (Eje z)). `Otro uso de las RNN es el texto para poder mantener la consistencia gramatical`. Actualmente se ven reemplazadas por los `Transformers`\n",
    " - **Convolucionales** (Convolutional Neural Network) para tratar **imagenes** (4D tensores de samples,height,weight,channels)\n",
    "\n",
    "## Deep Learning y Deep vision - Redes convolucionales\n",
    "\n",
    "Las redes convolucionales han jugado un papel importante en la historia del aprendizaje profundo. Las redes neuronales convolucionales fueron algunos de los primeros modelos profundos en funcionar bien,\n",
    "mucho antes de que los modelos profundos arbitrarios se consideraran viables.\n",
    "\n",
    "Las redes convolucionales también fueron de las primeras redes neuronales que resolvieron aplicaciones comerciales importantes y permanecen a la vanguardia de las aplicaciones comerciales de aprendizaje profundo en la actualidad. Por ejemplo, en la década de 1990, el grupo de investigación de redes neuronales de **AT&T** desarrolló una red convolucional para leer cheques bancarios.\n",
    "\n",
    "A finales de la década de 1990, este sistema implementado por **NCR** leía más del diez por ciento de todos los cheques en los Estados Unidos. Más tarde, varios sistemas de reconocimiento de escritura y OCR basados en redes convolucionales fueron implementados por **Microsoft**.\n",
    "\n",
    "\n",
    "Las redes convolucionales también se utilizaron para ganar muchos concursos. El interés comercial en el aprendizaje profundo comenzó cuando Krizhevsky, Sutskever y Hinton (2012) ganaron el desafío de reconocimiento de objetos **ImageNet**, pero es cierto que las redes convolucionales ya se habían utilizado para ganar otros concursos de aprendizaje automático y visión artificial con menos impacto años antes.\n",
    "\n",
    "Las redes convolucionales fueron de las primeras redes profundas en ser entrenadas mediante retropropagación.\n",
    "No está del todo claro por qué las redes convolucionales tuvieron éxito cuando se consideró que las redes neuronales generales fallaron en sus entrenamientos con retropropagación. Puede ser simplemente que las redes convolucionales fueran más eficientes computacionalmente que las redes totalmente conectadas o perceptrones multicapa, por lo que fue más fácil realizar múltiples experimentos con ellas y ajustar su\n",
    "implementación e hiperparámetros.\n",
    "\n",
    "\n",
    "![3](https://github.com/al34n1x/DataScience/blob/master/8.Machine_Learning/img/img_1.png?raw=true)\n",
    "\n",
    "Con el hardware actual, grandes redes totalmente conectadas parecen funcionar razonablemente en muchas tareas, incluso cuando se utilizan conjuntos de datos que estaban disponibles y funciones de activación que eran populares durante los tiempos en que se creía que estas redes no funcionaban bien. Puede serque las principales barreras para el éxito de las redes neuronales fueran psicológicas (los profesionales no\n",
    "esperaban que las redes neuronales funcionaran, por lo que no hicieron un esfuerzo serio para usarlas).\n",
    "\n",
    "Sea cual fuere el caso, es una suerte que las redes convolucionales hayan funcionado bien desde hace décadas. \n",
    "En muchos sentidos, llevaron el peso del aprendizaje profundo y allanaron el camino para la aceptación de las redes neuronales en general.\n",
    "\n",
    "\n",
    "![](./img/cnn.png)\n",
    "\n",
    "**Las redes convolucionales están especializadas para trabajar con datos que tienen una topología claramente estructurada en cuadrícula y para escalar dichos modelos a un tamaño muy profundo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHVBJ0Tx2WD4"
   },
   "source": [
    "## Convolución\n",
    "\n",
    "- Dense layers aprenden patrones globales (en toda la imagen)\n",
    "- Convoluciones detectan patrones locales (resistente a traslaciones)\n",
    "- Convoluciones permiten aprender jerarquías de patrones (patrones locales como lineas, curvas, etc. a círculos, rectangulos, a constelaciones)\n",
    "\n",
    "\n",
    "### Cómo se logra?\n",
    "\n",
    "![](./img/how_cnn.png)\n",
    "\n",
    "Lo que esta más oscuro son los filtros - `(El Kernel del Filtro)`\n",
    "\n",
    "Por ejemplo, el 19 del cuadro de la derecha viene de hacer:\n",
    "\n",
    "`0 * 1 + 3 * 1 + 1 * 2 + 2 * 2 + 2 * 2 + 3 * 0 + 0 * 0 + 2 * 1 + 2 * 2 = 19`\n",
    "\n",
    "\n",
    "- Divide el input (3D tensor) en parches y aplica la operación convolución (la misma en cada capa) a cada parche\n",
    "\n",
    "- Output (3D tensor) es un mapa de features o **mapa de activaciones** (cada una el resultado de aplicar la transformacion). Dicho mapa tendra dimensiones **Height x Width x Nº filtros**. Cada capa del volumen (eje z) es el resultado de aplicar un filtro a cada parche del input. En la capa de convolución, hay varios filtros, eso significa que debo de parametrizar ese valor. Las W pasan a estar dentro de los filtros.\n",
    "\n",
    "- Cada mapa de activación se traslada al siguiente layer convolucional que se encarga de encontrar otros mapas de activaciones.\n",
    "\n",
    "- A medida que vamos subiendo en las jerarquía de los bloques convolucionales, los filtros se van a ir especializando. Se van a activar patrones más sofisticados.\n",
    "\n",
    "- La convolución desliza cada parche sobre el input, parando en cada posible posicion y aplicando la transformacion (función kernel)\n",
    "\n",
    "\n",
    "### Mapa de activación para RGB\n",
    "\n",
    "![](./img/cnn_rgb_1.png)\n",
    "\n",
    "\n",
    "Por cada filtro vamos a tener solo un mapa de activación donde se suman.\n",
    "\n",
    "![](./img/cnn_rgb_2.png)\n",
    "\n",
    "Además se suma el BIAS\n",
    "\n",
    "![](./img/cnn_rgb_3.png)\n",
    "\n",
    "\n",
    "![](./img/1_capa_cnn.png)\n",
    "\n",
    "\n",
    "![](./img/2_capa_cnn.png)\n",
    "\n",
    "## Filtros y mapa de caracteristicas\n",
    "\n",
    "A medida que vamos avanzando en la extracción de características, las capas pueden aprender formas y patrones más complejos.\n",
    "\n",
    "![](./img/filtros.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "La siguiente lista muestra cómo se ve una `convnet` básica. Es una pila de capas `Conv2D` y `MaxPooling2D`. Verás en un minuto exactamente lo que hacen. Construiremos el modelo usando la API funcional.\n",
    "\n",
    "```\n",
    ">>> model.summary()\n",
    "\n",
    "Model: \"model\" \n",
    "\n",
    "_________________________________________________________________\n",
    "\n",
    "Layer (type)                 Output Shape              Param # \n",
    "\n",
    "================================================================= \n",
    "\n",
    "input_1 (InputLayer)         [(None, 28, 28, 1)]       0 \n",
    "\n",
    "_________________________________________________________________\n",
    "\n",
    "conv2d (Conv2D)              (None, 26, 26, 32)        320 \n",
    "\n",
    "_________________________________________________________________\n",
    "\n",
    "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0 \n",
    "\n",
    "_________________________________________________________________\n",
    "\n",
    "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496 \n",
    "\n",
    "_________________________________________________________________\n",
    "\n",
    "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0 \n",
    "\n",
    "_________________________________________________________________\n",
    "\n",
    "conv2d_2 (Conv2D)            (None, 3, 3, 128)         73856 \n",
    "\n",
    "_________________________________________________________________\n",
    "\n",
    "flatten (Flatten)            (None, 1152)              0 \n",
    "\n",
    "_________________________________________________________________\n",
    "\n",
    "dense (Dense)                (None, 10)                11530 \n",
    "\n",
    "=================================================================\n",
    "\n",
    "Total params: 104,202 \n",
    "\n",
    "Trainable params: 104,202 \n",
    "\n",
    "Non-trainable params: 0 \n",
    "\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ver que la salida de cada capa `Conv2D` y `MaxPooling2D` es un tensor de forma de rango 3 (alto, ancho, canales). Las dimensiones de ancho y alto tienden a reducirse a medida que se profundiza en el modelo. La cantidad de canales está controlada por el primer argumento pasado a las capas de `Conv2D (32, 64 o 128)`.\n",
    "\n",
    "Después de la última capa `Conv2D`, terminamos con una salida de forma `(3, 3, 128)`: un mapa de características de `3 × 3 de 128 canales`. El siguiente paso es introducir esta salida en un clasificador densamente conectado como los que ya conoces: una pila de capas densas. Estos clasificadores procesan vectores, que son `1D`, mientras que la salida actual es un tensor de rango 3. Para cerrar la brecha, aplanamos las salidas `3D` a `1D` con una capa Aplanar antes de agregar las capas Densa.\n",
    "\n",
    "Finalmente, hacemos una clasificación de `10 vías`, por lo que nuestra última capa tiene `10 salidas` y una activación `softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferencia entre Dense y Conv2d\n",
    "\n",
    "La diferencia fundamental entre una capa densamente conectada y una capa convolucional es la siguiente: \n",
    "- las capas densas aprenden patrones globales en su espacio de características de entrada (por ejemplo, para un dígito MNIST, patrones que involucran a todos los píxeles)\n",
    "\n",
    "- las capas convolucionales aprenden patrones locales, en el caso de imágenes, patrones encontrados en pequeñas ventanas 2D de las entradas. En el ejemplo anterior, estas ventanas eran todas de 3 × 3.\n",
    "\n",
    "![](./img/cnn_mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta característica clave le da a los `convnets` dos propiedades interesantes:\n",
    "\n",
    "- Los patrones que aprenden son invariantes en la traducción. Después de aprender un determinado patrón en la esquina inferior derecha de una imagen, un convnet puede reconocerlo en cualquier lugar: por ejemplo, en la esquina superior izquierda. Un modelo densamente conectado tendría que aprender el patrón de nuevo si apareciera en una nueva ubicación. Esto hace que los convnets sean eficientes en términos de datos al procesar imágenes (porque el mundo visual es fundamentalmente invariante a la traducción): necesitan menos muestras de entrenamiento para aprender representaciones que tengan poder de generalización.\n",
    "\n",
    "- Pueden aprender jerarquías espaciales de patrones. Una primera capa de convolución aprenderá pequeños patrones locales, como bordes, una segunda capa de convolución aprenderá patrones más grandes formados por las características de las primeras capas, y así sucesivamente. Esto permite a los convnets aprender de manera eficiente conceptos visuales cada vez más complejos y abstractos, porque el mundo visual es fundamentalmente espacialmente jerárquico.\n",
    "\n",
    "![](./img/cnn_cat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las capas de Keras `Conv2D`, estos parámetros son los primeros argumentos pasados a la capa: \n",
    "\n",
    "- `Conv2D` (profundidad_salida, (altura_ventana, ancho_ventana)).\n",
    "\n",
    "- Una convolución funciona deslizando estas ventanas de tamaño `3 × 3` o `5 × 5` sobre el mapa de características de entrada `3D`, deteniéndose en cada ubicación posible y extrayendo el parche `3D` de las características circundantes (forma (`alto_ventana`, `ancho_ventana`, `profundidad_entrada`)). \n",
    "\n",
    "- Luego, cada parche `3D` se transforma en un vector de forma `1D` (`profundidad_de_salida`), lo cual se realiza mediante un producto tensorial con una matriz de pesos aprendida, llamado núcleo de convolución: el mismo núcleo se reutiliza en cada parche. Todos estos vectores (uno por parche) luego se vuelven a ensamblar espacialmente en un mapa de forma de salida `3D` (`alto`, `ancho`, `salida_profundidad`). \n",
    "\n",
    "- Cada ubicación espacial en el mapa de características de salida corresponde a la misma ubicación en el mapa de características de entrada (por ejemplo, la esquina inferior derecha de la salida contiene información sobre la esquina inferior derecha de la entrada). Por ejemplo, con ventanas de `3 × 3`, la salida del vector `[i, j, :]` proviene de la entrada del parche `3D` `[i-1:i+1, j-1:j+1, :]`. El proceso completo se detalla en la figura.\n",
    "\n",
    "![](./img/how_cnn_2.png)\n",
    "\n",
    "## Combinación CNN y MLP\n",
    "\n",
    "![](./img/cnn_mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRoU6uh8NM1W",
    "outputId": "c06ae2b3-dcc5-437f-f5f4-53c8e5931e70"
   },
   "outputs": [],
   "source": [
    "# SOLO PARA USO EN GOOGLE COLABORATORY\n",
    "# Para conectar el notebook con la cuenta de gdrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3WiSfejBNCg"
   },
   "outputs": [],
   "source": [
    "BASE_FOLDER = '/content/drive/MyDrive/Senpai/Data Science 2023/6.Deep Learning/1.Course Content/data' # Se debe garantizar que la carpeta docencia compartida se almacena en el directorio raíz de Google Drive. En caso contrario modificar este path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTlHf22PigwY"
   },
   "source": [
    "## **INTRODUCCIÓN A LAS CONVOLUTIONAL NEURAL NETWORKS: MNIST DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHzrsKHQDMmQ"
   },
   "source": [
    "#### **- Cargando el conjunto de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGzZ11xaNHip",
    "outputId": "a21eaa42-6142-4126-ffd2-1472100d166f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#print(x_train.shape)\n",
    "#print(y_train.shape)\n",
    "#print(x_test.shape)\n",
    "#print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJr2bhE4DZYk"
   },
   "source": [
    "#### **- Acondicionando el conjunto de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peYCCxEbDZ44",
    "outputId": "d9902dcb-ed34-46f5-d81f-5f2bd2124010"
   },
   "outputs": [],
   "source": [
    "# Pre-procesado obligatorio cuando trabajo con redes neuronales\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.backend import expand_dims\n",
    "\n",
    "x_train, x_te = x_train / 255.0, x_test / 255.0 #Cambio al rango 0-1 -> Disminuyo CC\n",
    "print(y_train[0])\n",
    "#y_train = to_categorical(y_train, num_classes=10) #One-hot encoding para minimizar error\n",
    "#y_te = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "\"\"\"\n",
    "Como no voy a utilizar el one-hot, debo de usar sparse_categorical_crossentropy\n",
    "\"\"\"\n",
    "\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42) # 3 subconjuntos es de vital importancia\n",
    "#Expandir dimensiones porque en CNN tengo que especificar el número de canales\n",
    "print(x_tr.shape)\n",
    "\n",
    "\"\"\"\n",
    "Le expando en axis=3 para poder mantenerlo en canal gris.\n",
    "\"\"\"\n",
    "\n",
    "x_tr = expand_dims(x_tr, axis=3)\n",
    "x_val = expand_dims(x_val, axis=3)\n",
    "x_te = expand_dims(x_te, axis=3)\n",
    "print(x_tr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMztv3a5Kqou"
   },
   "source": [
    "#### **- Creando la topología de Red Neuronal (CNN) y entrenándola**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gp3kAblVNHi0"
   },
   "outputs": [],
   "source": [
    "# Construccion de una red CNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "# Red feedforward API secuencial\n",
    "convnet = Sequential()\n",
    "\n",
    "\"\"\"\n",
    "Como se que el problema es solucionable, y como las imágenes vienen bien formadas,\n",
    "ingreso 3 bloques convulocionales en el Base model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# BASE MODEL\n",
    "convnet.add(layers.Conv2D(32,(3,3),input_shape=(28,28,1),activation='relu'))\n",
    "convnet.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "convnet.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "convnet.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "convnet.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "\n",
    "#TOP MODEL\n",
    "convnet.add(layers.Flatten())\n",
    "convnet.add(layers.Dense(64,activation='relu'))\n",
    "convnet.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1WwGZQwNHi7",
    "outputId": "a6ad2a8f-66d3-4fde-900a-a793faa763b7"
   },
   "outputs": [],
   "source": [
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEZ4HAfGNHjB"
   },
   "outputs": [],
   "source": [
    "convnet.compile(optimizer='adam',\n",
    "               loss='sparse_categorical_crossentropy', #If labels are integers\n",
    "               #loss='categorical_crossentropy', #If labels are one-hot encoded\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "myf0nixeNHjH",
    "outputId": "247b37c2-a2ad-4985-d153-ca1c2737b90b"
   },
   "outputs": [],
   "source": [
    "H = convnet.fit(x_tr, y_tr, epochs=5, batch_size=128, validation_data=(x_val, y_val))\n",
    "\n",
    "\"\"\"\n",
    "batch_size de 128 imágenes por epoch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj3IHdxIKzpe"
   },
   "source": [
    "#### **- Observando el proceso de entrenamiento para tomar decisiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "R4umol-sJ7zw",
    "outputId": "23109b5f-c090-4ce6-b4af-d3b0b49443de"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Muestro gráfica de accuracy y losses\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 5), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 5), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 5), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 5), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGLEEnHqK5yW"
   },
   "source": [
    "#### **- Probando el conjunto de datos en el subset de test y evaluando el performance del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JypGcWmNHjM",
    "outputId": "5baaecdc-568c-495d-90a0-b8da63ce2c5c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Evaluando el modelo de predicción con las imágenes de test\n",
    "print(\"[INFO]: Evaluando red neuronal...\")\n",
    "predictions = convnet.predict(x_te, batch_size=128)\n",
    "#print(y_te[0])\n",
    "#print(predictions[0])\n",
    "print(classification_report(y_test, predictions.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xgqejLMinyM"
   },
   "source": [
    "## **¿POR QUE CONVOLUTIONAL NEURAL NETWORKS?: CIFAR DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yhEXcdHi4Aj"
   },
   "source": [
    "#### **- Cargando el conjunto de datos y acondicionándolo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lNcO0aXZiygR",
    "outputId": "66b670bb-8859-4ea5-d3c6-017b1c7d50fe"
   },
   "outputs": [],
   "source": [
    "# Importando el set de datos CIFAR10\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "print(\"[INFO]: Loading CIFAR-10 data...\")\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype(\"float\") / 255.0\n",
    "testX = testX.astype(\"float\") / 255.0\n",
    "labelNames = [\"Avión\", \"Automóvil\", \"Pájaro\", \"Gato\", \"Ciervo\", \"Perro\", \"Rana\", \"Caballo\", \"Barco\", \"Camión\"]\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "\n",
    "# Por si es necesario convertir a one-hot encoding\n",
    "#lb = LabelBinarizer()\n",
    "#trainY = lb.fit_transform(trainY)\n",
    "#testY = lb.transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8gD3BzEPVGM",
    "outputId": "d2e85188-244e-4360-e016-c4f484bb3164"
   },
   "outputs": [],
   "source": [
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlyJFsn_jHfj"
   },
   "source": [
    "#### **- Inspeccionando el conjunto de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "id": "-QQPADCujG_x",
    "outputId": "d8fb1b68-461a-4a23-f335-bf0bbe1f724b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "for n in range(1, 29):\n",
    "    fig.add_subplot(4, 7, n)\n",
    "    img = trainX[n]\n",
    "    plt.imshow(img)\n",
    "    plt.title(labelNames[trainY[n][0]])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zbIKienjdhb"
   },
   "source": [
    "#### **- Creando la topología de red neuronal y entrenándola: MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LW5UawlPj3zy",
    "outputId": "add17a6d-1a76-48df-c8e8-961a9b975e3b"
   },
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Arquitectura de red\n",
    "# Definimos el modo API Sequential\n",
    "model = Sequential() #(X)\n",
    "model.add(Flatten()) #(X)\n",
    "# Primera capa oculta\n",
    "model.add(Dense(2048, input_shape=(32*32*3,), activation=\"relu\")) #(X)\n",
    "#model.add(Dropout(0.5))\n",
    "# Segunda capa oculta\n",
    "model.add(Dense(1024, activation=\"relu\")) #(X)\n",
    "#model.add(Dropout(0.5))\n",
    "# Tercera capa oculta\n",
    "model.add(Dense(512, activation=\"relu\")) #(X)\n",
    "#model.add(Dropout(0.5))\n",
    "# Cuarta capa oculta\n",
    "model.add(Dense(128, activation=\"relu\")) #(X)\n",
    "#model.add(Dropout(0.5))\n",
    "# Quinta capa oculta\n",
    "model.add(Dense(32, activation=\"relu\")) #(X)\n",
    "# Capa de salida\n",
    "model.add(Dense(10, activation=\"softmax\")) #(X)\n",
    "\n",
    "\n",
    "# Compilamos el modelo y entrenamos\n",
    "print(\"[INFO]: Entrenando red neuronal...\")\n",
    "# Compilamos el modelo\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=SGD(0.01), metrics=[\"accuracy\"]) # Etiquetas en decimal #(X)\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=SGD(0.01), metrics=[\"accuracy\"]) # Etiquetas binarias #(X)\n",
    "# Entrenamos el perceptrón multicapa\n",
    "H = model.fit(trainX, trainY, validation_split=0.2, epochs=50, batch_size=32) #(X)\n",
    "\n",
    "# Evaluamos con las muestras de test\n",
    "print(\"[INFO]: Evaluando modelo...\")\n",
    "# Efectuamos predicciones\n",
    "predictions = model.predict(testX, batch_size=32) #(X)\n",
    "# Obtenemos el report\n",
    "print(classification_report(testY, predictions.argmax(axis=1), target_names=labelNames)) # Etiquetas en decimal #(X)\n",
    "# print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)) # Etiquetas binarias\n",
    "\n",
    "# Mostramos gráfica de accuracy y losses\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 50), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 50), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 50), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 50), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QfK8aj0klct"
   },
   "source": [
    "#### **- Creando la topología de red neuronal y entrenándola: CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zLCc7UJyknNP",
    "outputId": "dd612f7d-d956-4af4-bd23-be3c7a30e2ca"
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n",
    "\n",
    "n_epochs=50\n",
    "#########################################\n",
    "###### Definimos la arquitectura ########\n",
    "#########################################\n",
    "#BASE MODEL\n",
    "# Definimos entradas\n",
    "inputs = Input(shape=(trainX.shape[1], trainX.shape[2], trainX.shape[3]))\n",
    "\n",
    "# Primer set de capas CONV => RELU => CONV => RELU => POOL\n",
    "x1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(x1)\n",
    "x1 = BatchNormalization()(x1) # puedo codificar arquitecturas secuenciales con la funcional\n",
    "x1 = MaxPooling2D(pool_size=(2, 2))(x1)\n",
    "x1 = Dropout(0.25)(x1)\n",
    "\n",
    "# Segundo set de capas CONV => RELU => CONV => RELU => POOL\n",
    "x2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(x1) #(X)\n",
    "x2 = BatchNormalization()(x2) #(X)\n",
    "x2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(x2) #(X)\n",
    "x2 = BatchNormalization()(x2) #(X)\n",
    "x2 = MaxPooling2D(pool_size=(2, 2))(x2) #(X)\n",
    "x2 = Dropout(0.25)(x2) #(X)\n",
    "\n",
    "# Tercer set de capas CONV => RELU => CONV => RELU => POOL\n",
    "x3 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(x2) #(X)\n",
    "x3 = BatchNormalization()(x3) #(X)\n",
    "x3 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(x3) #(X)\n",
    "x3 = BatchNormalization()(x3) #(X)\n",
    "x3 = MaxPooling2D(pool_size=(2, 2))(x3) #(X)\n",
    "x3 = Dropout(0.25)(x3) #(X)\n",
    "\n",
    "# TOP MODEL\n",
    "# Primer (y único) set de capas FC => RELU\n",
    "xfc = Flatten()(x3) #(X)\n",
    "xfc = Dense(512, activation=\"relu\")(xfc) #(X)\n",
    "xfc = BatchNormalization()(xfc) #(X)\n",
    "xfc = Dropout(0.5)(xfc) #(X)\n",
    "# Clasificador softmax\n",
    "predictions = Dense(10, activation=\"softmax\")(xfc) #(X)\n",
    "\n",
    "# Unimos las entradas y el modelo mediante la función Model con parámetros inputs y ouputs (Consultar la documentación)\n",
    "model_cnn = Model(inputs=inputs, outputs=predictions) #(X)\n",
    "\n",
    "# Compilar el modelo\n",
    "print(\"[INFO]: Compilando el modelo...\")\n",
    "model_cnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(lr=0.001,beta_1=0.9, beta_2=0.999, epsilon=1e-08), metrics=[\"accuracy\"]) #(X)\n",
    "\n",
    "# Entrenamiento de la red\n",
    "print(\"[INFO]: Entrenando la red...\")\n",
    "H = model_cnn.fit(trainX, trainY, validation_split=0.2, batch_size=128, epochs=n_epochs, verbose=1) #(X)\n",
    "\n",
    "# Almaceno el modelo en Drive\n",
    "# Montamos la unidad de Drive\n",
    "drive.mount('/content/drive') #(X)\n",
    "# Almacenamos el modelo empleando la función mdoel.save de Keras\n",
    "model_cnn.save(BASE_FOLDER+\"deepCNN_CIFAR10.h5\") #(X)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(\"[INFO]: Evaluando el modelo...\")\n",
    "# Efectuamos la predicción (empleamos el mismo valor de batch_size que en training)\n",
    "predictions = model_cnn.predict(testX, batch_size=128) #(X)\n",
    "# Sacamos el report para test\n",
    "print(classification_report(testY, predictions.argmax(axis=1), target_names=labelNames)) #(X)\n",
    "\n",
    "# Gráficas\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, n_epochs), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, n_epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, n_epochs), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, n_epochs), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSb1uTpJn_eC"
   },
   "source": [
    "## **REDUCIENDO OVERFITTING MEDIANTE DATA AUGMENTATION**\n",
    "\n",
    "La mejor manera de hacer que un modelo de aprendizaje automático generalice mejor es entrenarlo con más datos. Por supuesto, en la práctica, la cantidad de datos que tenemos es limitada. Una forma de solucionar este problema es crear datos sintéticos y agregarlos al conjunto de datos de entrenamiento. Para algunas tareas de aprendizaje automático, es razonablemente sencillo crear nuevos datos sintéticos.\n",
    "\n",
    "Este enfoque es más sencillo cuando se trata de resolver una tarea de clasificación. Un clasificador necesita tomar una entrada x de múltiples dimensiones y mapearla a una identidad de categoría única y.\n",
    "\n",
    "Esto significa que la tarea principal que enfrenta un clasificador es ser invariable para una amplia variedad de transformaciones. Podemos generar nuevos pares (x, y) fácilmente, simplemente hay que transformar las entradas x en nuestro conjunto de entrenamiento.\n",
    "\n",
    "El aumento del conjunto de datos ha sido una técnica particularmente efectiva para un problema de clasificación específico: el reconocimiento de objetos. Las imágenes son de alta dimensionalidad y se caracterizan por un enorme grado de variabilidad, y muchos de estos efectos de variabilidad se pueden simular fácilmente. Las operaciones como realizar una translación de unos pocos píxeles en diferentes direcciones sobre las imágenes de entrenamiento a menudo pueden mejorar en gran medida la generalización, incluso si el modelo ya ha sido diseñado para ser parcialmente invariante a la translación mediante el uso de las técnicas de convolución y agregación.\n",
    "\n",
    "- Permite incrementar el número de ejemplos para reducir el overfitting (junto con el dropout)\n",
    "- Generar datos a partir de los presentes, a través de transformaciones geométricas y de intensidad:\n",
    "\t- Escalado\n",
    "\t- Zoom\n",
    "\t- Traslaciones\n",
    "\t- Rotaciones\n",
    "\t- Espejo\n",
    "\t- Trasformaciones de color\n",
    "\t- Histogram equialization\n",
    "\n",
    "- Para implementarlo en la práctica se hace uso de `ImageDataGenerators`\n",
    "\n",
    "[ImageDataGenerators](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n",
    "\n",
    "El ImageDataGenerator implementa las siguientes funciones:\n",
    "- Escalado\n",
    "- Zoom\n",
    "- Traslaciones\n",
    "- Rotaciones\n",
    "- Espejo\n",
    "- Trasformaciones de color\n",
    "- Histogram equialization\n",
    "\n",
    "Me genero un objeto `ImageDataGenerator` con los parámetros que deseo alterar\n",
    "de las imágenes.\n",
    "\n",
    "\n",
    ">NOTE: **No se debe realizar data augmentation sobre datos de validación o test.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSXhGQa3PQKc"
   },
   "source": [
    "#### **- Acondicionando dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zUYUzxuFOg9",
    "outputId": "441101b1-ae05-489f-b21c-2514d80f0d3f"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# Por si es necesario convertir a one-hot encoding\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "print(trainY.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEgHlw7oPSl6"
   },
   "source": [
    "#### **- Creando un contenedor DataGenerator para el aumento automático de muestras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FA0swBO3NHlQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\"\"\"\n",
    "Tengo que parametrizar bien los valores para evitar que las imágenes sintéticas no\n",
    "se vean demasiado impactadas en las transformaciones respecto el label.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15, # grados de rotacion aleatoria\n",
    "    width_shift_range=0.2, # fraccion del total (1) para mover la imagen\n",
    "    height_shift_range=0.2, # fraccion del total (1) para mover la imagen\n",
    "    horizontal_flip=True, # girar las imagenes horizontalmente (eje vertical)\n",
    "    # shear_range=0, # deslizamiento\n",
    "    zoom_range=0.2, # rango de zoom\n",
    "    # fill_mode='nearest', # como rellenar posibles nuevos pixeles\n",
    "    # channel_shift_range=0.2 # cambios aleatorios en los canales de la imagen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jiAe2cEPk0Y"
   },
   "source": [
    "#### **- Inspeccionando las muestras generadas sintéticamente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "id": "vtgZCInwNHlX",
    "outputId": "b93c40d2-1e5e-43ee-865e-d0dd3158d539"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample = 13\n",
    "plt.imshow(image.array_to_img(trainX[sample]))\n",
    "plt.show()\n",
    "print('Label = {}'.format(labelNames[trainY[sample].argmax(axis=0)]))\n",
    "\n",
    "fig, axes = plt.subplots(2,2)\n",
    "i = 0\n",
    "for batch in datagen.flow(trainX[sample].reshape((1,32,32,3)),batch_size=1):\n",
    "    #plt.figure(i)\n",
    "    axes[i//2,i%2].imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i == 4:\n",
    "        break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RCjtfY8Ps-G"
   },
   "source": [
    "#### **- Creando la topología de red neuronal y entrenándola: CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jgy3NVSx8y7x",
    "outputId": "1c03729c-fa98-490b-ecc5-69ac40adbdbf"
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n",
    "\n",
    "#########################################\n",
    "###### Definimos la arquitectura ########\n",
    "#########################################\n",
    "# Definimos entradas\n",
    "inp = Input(shape=(trainX.shape[1], trainX.shape[2], trainX.shape[3]))\n",
    "\n",
    "# Primer set de capas CONV => RELU => CONV => RELU => POOL\n",
    "x1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(inp)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = MaxPooling2D(pool_size=(2, 2))(x1)\n",
    "x1 = Dropout(0.25)(x1)\n",
    "\n",
    "# Segundo set de capas CONV => RELU => CONV => RELU => POOL\n",
    "x2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(x1)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "x2 = Dropout(0.25)(x2)\n",
    "\n",
    "# Segundo set de capas CONV => RELU => CONV => RELU => POOL\n",
    "x2 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "x2 = Dropout(0.25)(x2)\n",
    "\n",
    "# Primer (y único) set de capas FC => RELU\n",
    "xfc = Flatten()(x2)\n",
    "xfc = Dense(512, activation=\"relu\")(xfc)\n",
    "xfc = BatchNormalization()(xfc)\n",
    "xfc = Dropout(0.5)(xfc)\n",
    "# Clasificador softmax\n",
    "predictions = Dense(10, activation=\"softmax\")(xfc)\n",
    "\n",
    "# Unimos las entradas y el modelo mediante la función Model con parámetros inputs y ouputs (Consultar la documentación)\n",
    "model_aug = Model(inputs=inp, outputs=predictions)\n",
    "\n",
    "# Compilar el modelo\n",
    "print(\"[INFO]: Compilando el modelo...\")\n",
    "model_aug.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08), metrics=[\"accuracy\"])\n",
    "\n",
    "# Entrenamiento de la red\n",
    "print(\"[INFO]: Entrenando la red...\")\n",
    "H_aug = model_aug.fit(datagen.flow(trainX, trainY, batch_size=128),\n",
    "                                steps_per_epoch = len(trainX)*2/ 128, epochs=50, validation_data=(testX, testY))\n",
    "# Almaceno el modelo en Drive\n",
    "# Montamos la unidad de Drive\n",
    "drive.mount('/content/drive')\n",
    "# Almacenamos el modelo empleando la función mdoel.save de Keras\n",
    "model_aug.save(BASE_FOLDER+\"deepCNN_CIFAR10_aug.h5\")\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(\"[INFO]: Evaluando el modelo...\")\n",
    "# Efectuamos la predicción (empleamos el mismo valor de batch_size que en training)\n",
    "predictions = model_aug.predict(testX, batch_size=128)\n",
    "# Sacamos el report para test\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames))\n",
    "\n",
    "# Gráficas\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(0, 50), H_aug.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(np.arange(0, 50), H_aug.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(np.arange(0, 50), H_aug.history[\"accuracy\"], label=\"train_acc\")\n",
    "# plt.plot(np.arange(0, 50), H_aug.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
