{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/al34n1x/DataScience/blob/master/3.Pandas/Pandas_DataLoading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7PfTJRnkzq1",
        "colab_type": "toc"
      },
      "source": [
        ">[Carga de datos en Panda](#scrollTo=nelHfwuqPmDH)\n",
        "\n",
        ">[Leer y escribir datos en formato de texto](#scrollTo=BWuTIsOpoywz)\n",
        "\n",
        ">>[Funciones](#scrollTo=vcc8_MSHtmtm)\n",
        "\n",
        ">>[Trabajando con valores faltantes](#scrollTo=NseoQfuAPzpQ)\n",
        "\n",
        ">>[Leer archivos de texto en partes](#scrollTo=lELr76GBRh1n)\n",
        "\n",
        ">>[Escribiendo Datos en formato texto](#scrollTo=r31ppoPyT6vS)\n",
        "\n",
        ">>[Formato CSV](#scrollTo=cRWqIex1hjLb)\n",
        "\n",
        ">>[JSON Data](#scrollTo=Fumt3vFx_CnF)\n",
        "\n",
        ">>[XML y HTML Web Scrapping](#scrollTo=RAFNinVHUZVx)\n",
        "\n",
        ">>[Formato HDF5](#scrollTo=The4cD8zbg4-)\n",
        "\n",
        ">>[Lectura desde archivos Microsoft Excel](#scrollTo=OOyHUIhxddJW)\n",
        "\n",
        ">>[Interactuando con APIs](#scrollTo=fTfC_zNBgRGY)\n",
        "\n",
        ">>[Interactuando con Base de Datos](#scrollTo=PdIyYXWNhY18)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nelHfwuqPmDH"
      },
      "source": [
        "# Carga de datos en Pandas\n",
        "\n",
        "Como hemos discutdo, acceder a los datos es un importantísimo primer paso proceder con su análisis. \n",
        "La entrada y salida de de datos generalmente se pueden dividir en las siguientes categorías, leer archivos de texto u otro formato (csv, tabulado, etc)cargar datos desde una base de datos, e interactuar con recursos de red como puede ser una página web o una interface API.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWuTIsOpoywz"
      },
      "source": [
        "# Leer y escribir datos en formato de texto\n",
        "\n",
        "Pandas posee muchas herramientas para leer archivos tabulados e insertarlos en un objeto dataframe. \n",
        "Algunas de las funciones con las que trabajarás son **read_csv** y **read_table** \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcc8_MSHtmtm"
      },
      "source": [
        "## Funciones \n",
        "\n",
        "Aquí algunas funciones ampliamente utilizadas for Pandas que serán de utilidad para el desarrollo de las actividades.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Función | Descripción\n",
        "--- | ---\n",
        "**read_csv** | Cargue datos delimitados de un archivo, URL u objeto similar a un archivo; usar coma como delimitador predeterminado\n",
        "**read_table** | Cargue datos delimitados de un archivo, URL u objeto similar a un archivo; use tab ('\\ t') como delimitador predeterminado\n",
        "**read_fwf** | \tLeer datos en formato de columna de ancho fijo, (es decir, sin delimitadores\n",
        "**read_clipboard** | Versión de read_table que lee datos del portapapeles; útil para convertir tablas de páginas web\n",
        "**read_excel** | Leer datos tabulares de un archivo Excel XLS o XLSX\n",
        "**read_hdf** | Leer archivos HDF5 escritos por pandas\n",
        "**read_html**| Leer todas las tablas encontradas en un documento HTML dado\n",
        "**read_json**\t| Leer datos de una representación de cadena JSON (JavaScript Object Notation)\n",
        "**read_msgpack**|Leer datos de pandas codificados con el formato binario MessagePack\n",
        "**read_pickle**|Leer un objeto arbitrario almacenado en formato Python pickle\n",
        "**read_sas**|Leer un conjunto de datos almacenado en uno de los formatos de almacenamiento personalizados del sistema SAS\n",
        "**read_sql**|Lea los resultados de una consulta SQL (usando SQLAlchemy) como un DataFrame de pandas\n",
        "**read_stata**|Leer un conjunto de datos del formato de archivo Stata\n",
        "**read_feather**|Lea el formato de archivo binario Feather\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8haeDTSxNWp"
      },
      "source": [
        "Los argumentos opcionales para estas funciones pueden caer en alguna de estás categorías:\n",
        "\n",
        "**Indexación**\n",
        "Puede tratar una o más columnas como el DataFrame devuelto, y si desea obtener nombres de columna del archivo, el usuario, o no obtenerlos.\n",
        "\n",
        "**Inferencia de tipos y conversión de datos**\n",
        "Esto incluye las conversiones de valores definidos por el usuario y la lista personalizada de marcadores de valores faltantes.\n",
        "\n",
        "**Análisis de fecha y hora**\n",
        "Incluye la capacidad de combinación, incluida la combinación de información de fecha y hora distribuida en varias columnas en una sola columna en el resultado.\n",
        "\n",
        "**Iterando**\n",
        "Soporte para iterar sobre fragmentos de archivos muy grandes.\n",
        "\n",
        "**Problemas de datos sucios/ruido**\n",
        "Saltar filas o un pie de página, comentarios u otras cosas menores como datos numéricos con miles separados por comas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_fK_SIpPZa1"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "file = os.path.join('./sample_data/california_housing_train.csv') #Define el path al archivo\n",
        "df = pd.read_csv(file)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dt0et2hFBv2"
      },
      "source": [
        "pd.read_table('./sample_data/california_housing_train.csv', sep=',') # También podemos usar read_table con un delimitador"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgliJuqYHUPn"
      },
      "source": [
        "Podemos eliminar los headers utilizando el parámetro correspondiente **header=none**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2wBnFv1FrpY"
      },
      "source": [
        "df = pd.read_csv('./sample_data/california_housing_train.csv', header=None, names=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrB0BaITGevp"
      },
      "source": [
        "df2 = df.drop(index=0) #Eliminamos la primera fila que nos había quedado con los headers originales \n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiSAA-qRH6_r"
      },
      "source": [
        "Podemos crear índices jerárquicos desde multiples columnas o nombres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9LKpx4NICi0"
      },
      "source": [
        "df3 = pd.read_csv('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/Herarchical_index.csv')\n",
        "df3 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqv7TEihLO-4"
      },
      "source": [
        "df_parsed = pd.read_csv('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/Herarchical_index.csv',\n",
        "                        index_col=['key1', 'key2'])\n",
        "df_parsed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzJ21nybNIZH"
      },
      "source": [
        "En algunas situaciones nos encontraremos con archivos que no estan delimitados por comas. en dichos casos, podemos pasar expresioner regulares como delimitadores de la función **read_table**. En el siguiente caso importaremos un archivo con espacios en blanco como delimitador  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kMcH2w6NHfh"
      },
      "source": [
        "df4 = pd.read_table('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/file_spaces.txt', sep='\\s+') #Le pasamos el delimitador\n",
        "df4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG-Z11e6PqEd"
      },
      "source": [
        "Que sucede cuando nos encontramos con archivos que poseen datos innecesarios? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQfVQGS-OrEc"
      },
      "source": [
        "df5 = pd.read_csv('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/file_comments.csv')\n",
        "df5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWWi0CATPAHq"
      },
      "source": [
        "df5 = pd.read_csv ('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/file_comments.csv', skiprows=[0,2,3])\n",
        "df5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NseoQfuAPzpQ"
      },
      "source": [
        "## Trabajando con valores faltantes\n",
        "\n",
        "El manejo de valores perdidos es una parte importante y frecuentemente matizada del proceso de análisis de archivos. Los datos que faltan generalmente no están presentes o están marcados por algún valor de referencia. Por defecto, Pandas usa un conjunto de referencia comunes, como NA y NULL:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9i_pKTNQASX"
      },
      "source": [
        "df6 = pd.read_csv('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/missing_values.csv')\n",
        "df6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0cqFOULQnqu"
      },
      "source": [
        "pd.isnull(df6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU2m-U2tROQR"
      },
      "source": [
        "También podemos agregar valores de referencia y anidarlos como diccionarios por columnas y filas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUq2d4D0Q2yU"
      },
      "source": [
        "sentinels = {'message': ['foo', 'NA'], 'something': ['two']}\n",
        "pd.read_csv('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/missing_values.csv', na_values=sentinels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lELr76GBRh1n"
      },
      "source": [
        "## Leer archivos de texto en partes\n",
        "\n",
        "Cuando procesamos archivos muy grandes o descubres el conjunto correcto de argumentos para procesar correctamente un archivo grande, es posible que solo desees leer en una pequeña parte de un archivo o iterar a través de fragmentos más pequeños del mismo.\n",
        "\n",
        "Antes de mirar un archivo grande, hacemos que la configuración de visualización de pandas sea más compacta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdnP-LujRwc9"
      },
      "source": [
        "pd.options.display.max_rows = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw1d0-yhRzIW"
      },
      "source": [
        "df7 = pd.read_csv('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/big_file.csv')\n",
        "df7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyyOhRQrSRp1"
      },
      "source": [
        "df7 = pd.read_csv('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/big_file.csv', nrows=5) #Definimos número de filas\n",
        "df7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YtXfps2SbxZ"
      },
      "source": [
        "Para poder leer un archivo en diferentes trozos, se debe especificar el parámetro **chunksize** como el número de filas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jr6R-ceSl94"
      },
      "source": [
        "import pandas as pd\n",
        "chunk = pd.read_csv('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/big_file.csv', chunksize=1000)\n",
        "chunk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in chunk:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "eSjGUPRVIt9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfkKvjomTQBl"
      },
      "source": [
        "tot = pd.Series([]) # Creamos un objeto serie\n",
        "for piece in chunk: #iteramos\n",
        "    tot = tot.add(piece['key'].value_counts(), fill_value=0) #vamos agregando los valores de chunk en la serie\n",
        "\n",
        "tot = tot.sort_values(ascending=False)\n",
        "tot[:10] # Mostramos los primeros 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r31ppoPyT6vS"
      },
      "source": [
        "## Escribiendo Datos en formato texto\n",
        "\n",
        "Los datos pueden ser exportados en formato delimitado, por ejemplo consideremos el siguiente archivo de lectura. Tanto los DataFrame como las Series permiten exportar el resultado con el método **.to_csv**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZVnAJVZT553"
      },
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/missing_values.csv')\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8gbTti9UpYI"
      },
      "source": [
        "Usando el método Dataframe a csv podemos escribir datos a archivos delimitados por coma.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8nuipMTUniB"
      },
      "source": [
        "data.to_csv ('./sample_data/missing_values_out.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQrtNCK8VGqg"
      },
      "source": [
        "!cat './sample_data/missing_values_out.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmmD2qLRVWEf"
      },
      "source": [
        "Podemos utilizar otros delimitadores utilizando el método Python **sys.stdout** para imprimir el resultado por consola."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgj74PtDVVku"
      },
      "source": [
        "import sys\n",
        "data.to_csv(sys.stdout, sep='|')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InIkQUV7g93S"
      },
      "source": [
        "Adicionalmente, se puede especificar nulos en aquellos valores faltantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NcmmreXWok4"
      },
      "source": [
        "data.to_csv(sys.stdout, na_rep='NULL')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRWqIex1hjLb"
      },
      "source": [
        "## Formato CSV\n",
        "\n",
        "Los archivos CSVs suelen venir en diferentes formatos. Se puede definir un formato con diferente delimitadores, convención de cadenas, o terminadores de lineas mediante la implementación de una sublcase. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR9-2zyyr9Cs"
      },
      "source": [
        "import csv\n",
        "class my_dialect(csv.Dialect):\n",
        "    lineterminator = '\\n'\n",
        "    delimiter = ','\n",
        "    quotechar = '\"'\n",
        "    quoting = csv.QUOTE_MINIMAL #add quote only when required\n",
        "f = open('./sample_data/missing_values_out.csv')\n",
        "reader = csv.reader(f, dialect=my_dialect)\n",
        "\n",
        "for line in reader:\n",
        "  print(line)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3ITwZSORMFl"
      },
      "source": [
        "Para escribir archivos delimitados de forma manual puedes usar el método **csv.writer**. Accepta los mismo tipos de archivos, dialectos y opciones de formato que el método **csv.reader**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEB2dM2MRjNz"
      },
      "source": [
        "with open('./sample_data/missing_values_out.csv', 'w') as f:\n",
        "    writer = csv.writer(f, dialect=my_dialect)\n",
        "    writer.writerow(('one', 'two', 'three'))\n",
        "    writer.writerow(('1', '2', '3'))\n",
        "    writer.writerow(('4', '5', '6'))\n",
        "    writer.writerow(('7', '8', '9'))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zljY0qjwR75G"
      },
      "source": [
        "!cat './sample_data/missing_values_out.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fumt3vFx_CnF"
      },
      "source": [
        "## JSON Data\n",
        "\n",
        "JSON (abreviatura de JavaScript Object Notation) se ha convertido en uno de los formatos estándar para enviar datos por solicitud HTTP entre navegadores web y otras aplicaciones. Es un formato de datos mucho más libre que un formulario de texto tabular como CSV. \n",
        "Al trabajar con archivos en format JSON puedes utilizar infindad de herramientas para evaluar la correcta redacción del código, por ejemplo [JSON Formatter](https://jsonformatter.curiousconcept.com/) \n",
        "Aquí hay un ejemplo:\n",
        "\n",
        "```\n",
        "{\n",
        "   \"nombre\":\"Wes\",\n",
        "   \"lugares_donde_vivio\":[\n",
        "      \"United States\",\n",
        "      \"Spain\",\n",
        "      \"Germany\"],\n",
        "\n",
        "   \"mascotas\":null,\n",
        "   \"hermanos\":[\n",
        "      {\n",
        "         \"nombre\":\"Willy\",\n",
        "         \"edad\":30,\n",
        "         \"mascotas\":[\n",
        "            \"Zeus\",\n",
        "            \"Zuko\"]\n",
        "      },\n",
        "      {\n",
        "         \"nombre\":\"Wonka\",\n",
        "         \"edad\":38,\n",
        "         \"mascotas\":[\n",
        "            \"Sixes\",\n",
        "            \"Stache\",\n",
        "            \"Cisco\"]\n",
        "      \n",
        "      }\n",
        "   \n",
        "    ]\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTHKYdHh-8M3"
      },
      "source": [
        "obj = \"\"\"\n",
        "{\"nombre\": \"Wes\",\n",
        " \"lugares_donde_vivio\": [\"United States\", \"Spain\", \"Germany\"],\n",
        " \"mascotas\": null,\n",
        " \"hermanos\": [{\"nombre\": \"Willy\", \"edad\": 30, \"mascotas\": [\"Zeus\", \"Zuko\"]},\n",
        "              {\"nombre\": \"Wonka\", \"edad\": 38,\n",
        "               \"mascotas\": [\"Sixes\", \"Stache\", \"Cisco\"]}]\n",
        "}\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhhCREOD_cy0"
      },
      "source": [
        "JSON es casi un código de Python válido con la excepción de su valor nulo y algunos otros matices (como no permitir las comas finales al final de las listas). \n",
        "Los tipos básicos son objetos, matrices, cadenas, números, booleanos y nulos. \n",
        "Todas las claves en un objeto deben ser cadenas. Hay varias bibliotecas de Python para leer y escribir datos JSON. Usaremos json aquí, ya que está integrado en la biblioteca estándar de Python. Para convertir una cadena JSON a Python, usa **json.loads**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocum61p4SK29"
      },
      "source": [
        "import json\n",
        "resultado = json.loads(obj) # importamos el objeto con los datos en formato json\n",
        "resultado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh8DH-RjScQm"
      },
      "source": [
        "El método **json.dumps** convierte un objeto Python a JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjwYS_rNSjYk"
      },
      "source": [
        "asjson = json.dumps(resultado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj4cOwmaS3Ph"
      },
      "source": [
        "Convertir un objeto JSON o una lista de objetos en un DataFrame o alguna estructura depende del desarrollador. \n",
        "Convenientemente, puede pasar una lista de dictados (que anteriormente eran objetos JSON) al constructor DataFrame y seleccionar un subconjunto de los campos de datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSaQMTtuTO4t"
      },
      "source": [
        "hermanos = pd.DataFrame(resultado['hermanos'], columns=['nombre', 'edad'])\n",
        "hermanos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U8HMI_oUM2T"
      },
      "source": [
        "Asimismo puedes leer un archivo en formato JSON y convertirlo directamente a un objeto DataFrame mediante el método **pd.read_json**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgUXK7EMT_-a"
      },
      "source": [
        "data = pd.read_json('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/ejemplo.json')\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAFNinVHUZVx"
      },
      "source": [
        "## XML y HTML Web Scrapping\n",
        "\n",
        "Python tiene muchas librerías para leer y escribir datos en formatos HTML y XML. Los ejemplos incluyen **lxml**, **Beautiful Soup** y **html5lib**. Si bien **lxml** es comparativamente mucho más rápido en general, las otras bibliotecas pueden manejar mejor los archivos HTML o XML con formato incorrecto.\n",
        "\n",
        "Pandas tiene una función incorporada, **read_html**, que usa librerías como **lxml** y **Beautiful Soup** para analizar automáticamente tablas de archivos HTML como objetos DataFrame. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eEzVniUU2IA"
      },
      "source": [
        "Para demostrar cómo funciona esto, descargaremos un archivo HTML  de la agencia gubernamental de la FDIC de los Estados Unidos que muestra las quiebras bancarias.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqkmJVjLU1hr"
      },
      "source": [
        "tablas = pd.read_html('https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/informacion_fdic.html')\n",
        "len(tablas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpLjqzdZVoAk"
      },
      "source": [
        "tablas[0].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMOB2DmGWbkz"
      },
      "source": [
        "**XML (eXtensible Markup Language)** es otro formato común de datos estructurados que admite datos jerárquicos y anidados con metadatos. \n",
        "\n",
        "Es similar a HTML en su apariencia, pero XML se usa para la presentación de datos, mientras que HTML se usa para definir qué datos se están usando. XML está diseñado exclusivamente para enviar y recibir datos entre clientes y servidores.\n",
        "\n",
        "Anteriormente, se mostró la función **pandas.read_html**, que usa lxml o Beautiful Soup debajo del capó para analizar datos de HTML. XML y HTML son estructuralmente similares, pero XML es más general. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcX2u_J0abc4"
      },
      "source": [
        "En el siguiente ejemplo tomaremos un DataSet del *New York Metropolitan Transportation Authority (MTA)* que publica información sobre los servicios de Bus y Trenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8litpqQXR6n"
      },
      "source": [
        "from urllib.request import urlopen #Importamos Librería de manejo de URL \n",
        "from xml.etree.ElementTree import parse #Importamos Librería de gestión de formato XML \n",
        "from lxml import objectify\n",
        "\n",
        "\n",
        "var_url = urlopen(\"https://raw.githubusercontent.com/al34n1x/DataScience/master/3.Pandas/mta_performance.xml\")\n",
        "xmldoc = objectify.parse(var_url)\n",
        "root = xmldoc.getroot()\n",
        "data = []\n",
        "\n",
        "skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ',\n",
        "               'DESIRED_CHANGE', 'DECIMAL_PLACES']\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64fKXJR1axD8"
      },
      "source": [
        "**root.INDICATOR** devuelve un generador para cada elemento <INDICADOR> XML. Para cada entrada, podemos obtener dicho diccionario de los tags como YTD_ACTUAL. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BSJGQFebCUl"
      },
      "source": [
        "for elt in root.INDICATOR:  # Iteramos sobre el archivo XML y los diferentes Tags\n",
        "    el_data = {}\n",
        "    for child in elt.getchildren():\n",
        "        if child.tag in skip_fields:\n",
        "            continue\n",
        "        el_data[child.tag] = child.pyval\n",
        "    data.append(el_data) # Agregamos contenidos en la variable data\n",
        "\n",
        "perf = pd.DataFrame(data) # Convertimos data a un DataFrame\n",
        "perf.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "The4cD8zbg4-"
      },
      "source": [
        "## Formato HDF5\n",
        "\n",
        "HDF5 es un formato de archivo ampliamente utilizado  destinado a almacenar grandes cantidades de datos científicos. Está disponible como una biblioteca C y tiene interfaces disponibles en muchos otros lenguajes, incluidos Java, Julia, MATLAB y Python. \n",
        "\"HDF\" en HDF5 significa formato de datos jerárquico. \n",
        "Cada archivo HDF5 puede almacenar múltiples conjuntos de datos y metadatos compatibles. En comparación con formatos más simples, HDF5 admite la compresión sobre la marcha con una variedad de modos de compresión, lo que permite que los datos con patrones repetidos se almacenen de manera más eficiente. HDF5 puede ser una buena opción para trabajar con conjuntos de datos muy grandes que no caben en memoria, ya que puede leer y escribir eficientemente pequeñas secciones de conjuntos mucho más grandes.\n",
        "\n",
        "Pandas proporciona una interfaz de alto nivel que simplifica el almacenamiento de objetos Series y DataFrame en dicho formato. La clase HDFStore funciona como un dict y maneja los detalles de bajo nivel:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0GyjWiCbYJ7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.DataFrame({'a': np.random.randn(100)})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOXR5lwu1CNh"
      },
      "source": [
        "almacenamiento = pd.HDFStore('./sample_data/mydata.h5')\n",
        "almacenamiento['obj1'] = df\n",
        "almacenamiento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_tDcvovce47"
      },
      "source": [
        "almacenamiento['obj1'] #Los objetos almacenados en HDF5 pueden ser obtenidos mediante el mismo dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igdbk72dcpzr"
      },
      "source": [
        "HDFStore soporta dos tipos de esquema de almaceiamiento **Fijo** y **Tabla**, siendo el último más lento, pero soporta operaciones de consulta usando syntax especial \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeOnApJFc60g"
      },
      "source": [
        "almacenamiento.put('obj2', df, format='table')\n",
        "almacenamiento.select('obj2', where=['index >= 10 and index <= 15'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqio1dUxdGGz"
      },
      "source": [
        "La función **pandas.read_hdf** provee un shortcut a estas herramientas \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4XVTrSBdSTW"
      },
      "source": [
        "df.to_hdf('./sample_data/mydata_2.h5', 'obj3', format='table')\n",
        "pd.read_hdf('./sample_data/mydata_2.h5', 'obj3', where=['index < 5'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOyHUIhxddJW"
      },
      "source": [
        "## Lectura desde archivos Microsoft Excel\n",
        "\n",
        "Pandas también admite la lectura de datos tabulares almacenados en archivos Excel 2003 (y superiores) utilizando la clase **ExcelFile** o la función **pandas.read_excel**. Internamente, estas herramientas utilizan los paquetes complementarios **xlrd** y **openpyxl** para leer archivos XLS y XLSX, respectivamente. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHYMVopdd4ay"
      },
      "source": [
        "xlsx = pd.ExcelFile('https://github.com/al34n1x/DataScience/blob/master/3.Pandas/sample.xlsx?raw=true')\n",
        "df = pd.read_excel(xlsx, 'Sheet1')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs44eZT9d3Y4"
      },
      "source": [
        "Para escribir datos Pandas a formato Excel debes primero crear un ExcelWriter, luego escribir los datos utilizando objetos Pandas a métodos excel\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrmyrLxDfbmb"
      },
      "source": [
        "writer = pd.ExcelWriter('./sample_data/sample_output.xlsx')\n",
        "df.to_excel(writer,'sheet1')\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTfC_zNBgRGY"
      },
      "source": [
        "## Interactuando con APIs\n",
        "\n",
        "Muchos sitios web tienen API públicas que proporcionan datos a través de JSON o algún otro formato. Hay varias formas de acceder a estas API desde Python; Un método fácil de usar que recomiendo es el paquete **requests**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VPzIg8nghL-"
      },
      "source": [
        "import requests\n",
        "'''\n",
        "La siguiente API nos da los últimos 30 issues de Pandas en Github.\n",
        "Podemos utilizar un método GET HTTP para poder hacernos de la información\n",
        "'''\n",
        "url = 'https://api.github.com/repos/pandas-dev/pandas/issues' \n",
        "resp = requests.get(url)\n",
        "resp\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfzdPYu3g9Mq"
      },
      "source": [
        "datos = resp.json()\n",
        "datos[0]['title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxqtLo1bhJHb"
      },
      "source": [
        "issues = pd.DataFrame(datos, columns=['number', 'title',\n",
        "                  'labels', 'state'])\n",
        "issues"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdIyYXWNhY18"
      },
      "source": [
        "## Interactuando con Base de Datos\n",
        "\n",
        "En un entorno empresarial, es posible que la mayoría de los datos no se almacenen en archivos de texto o Excel. Las bases de datos relacionales basadas en SQL (como Oracle, SQL Server, PostgreSQL y MySQL) se usan ampliamente, y muchas bases de datos alternativas se han vuelto bastante populares. \n",
        "\n",
        "Cargar datos de SQL en un DataFrame es bastante sencillo, y Pandas tiene algunas funciones para simplificar el proceso. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE7vJi1ShqWF"
      },
      "source": [
        "import sqlite3\n",
        "\n",
        "#Creamos una tabla en una base de datos sqlite3\n",
        "\n",
        "query = \"\"\"\n",
        "    CREATE TABLE test \n",
        "    ( a VARCHAR(20), \n",
        "      b VARCHAR(20),\n",
        "      c REAL, \n",
        "      d INTEGER\n",
        "    );\"\"\"\n",
        "\n",
        "con = sqlite3.connect('mydata.sqlite')\n",
        "con.execute(query)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ciMMy7OiCMJ"
      },
      "source": [
        "con.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ5k3PoViJGw"
      },
      "source": [
        "datos = [('Buenos Aires', 'La Plata', 1.25, 6),\n",
        "              ('Mendoza', 'Ciudad de Mendoza', 2.6, 3),\n",
        "              ('San Luis', 'San Luis', 1.7, 5)]\n",
        "\n",
        "\n",
        "query = \"INSERT INTO test VALUES(?, ?, ?, ?)\"\n",
        "\n",
        "con.executemany(query, datos)\n",
        "con.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbV4TRBCinRy"
      },
      "source": [
        "cursor = con.execute('select * from test')\n",
        "filas = cursor.fetchall()\n",
        "filas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1D2SufKi1o-"
      },
      "source": [
        "Esto es un poco molesto, asumimos que el desarrollador no prefiere repetir estos pasos cada vez que consulta la base de datos. \n",
        "El proyecto **SQLAlchemy** es un popular kit de herramientas Python SQL que abstrae muchas de las diferencias comunes entre las bases de datos SQL. \n",
        "Pandas tiene una función **read_sql** que le permite leer datos fácilmente desde una conexión SQLAlchemy general. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade 'sqlalchemy<2.0'"
      ],
      "metadata": {
        "id": "NNNB6pFnAxSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MS69x47jE19"
      },
      "source": [
        "import sqlalchemy as sqla\n",
        "db = sqla.create_engine('sqlite:///mydata.sqlite')\n",
        "'''\n",
        "Hacemos un query a la base de datos seleccionando todo \n",
        "en la tabla test y se lo asignamos al DataFrame df\n",
        "'''\n",
        "df = pd.read_sql('select * from test where b = \\'La Plata\\' ', db)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de clases de datos para crear modelos de base de datos en Python\n",
        "\n",
        "En Python, las clases de datos son una forma de definir clases simples para almacenar datos. Se definen utilizando el decorador `@dataclass` y el módulo dataclasses, que se introdujo en Python 3.7.\n",
        "\n",
        "Una clase de datos se define como una clase normal de Python pero con el decorador @dataclass aplicado."
      ],
      "metadata": {
        "id": "F6pJiDGO6upP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from sqlalchemy import Column, Integer, String\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy.orm import Session\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "\n",
        "Base = declarative_base()\n",
        "\n",
        "@dataclass\n",
        "class Person(Base):\n",
        "    __tablename__ = 'person'\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    name = Column(String)\n",
        "    age = Column(Integer)"
      ],
      "metadata": {
        "id": "s4vX8M3Y6ySs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = create_engine('sqlite:///test.db')\n",
        "Base.metadata.create_all(bind=engine)\n",
        "Session = sessionmaker(bind=engine)\n",
        "session = Session()"
      ],
      "metadata": {
        "id": "tq22h5ym7p4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = Person(name=\"John\", age=30)\n",
        "session.add(p1)\n",
        "p2 = Person(name=\"John2222\", age=3000)\n",
        "session.add(p2)\n",
        "session.commit()\n"
      ],
      "metadata": {
        "id": "RJQhKaDD7rUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_persons = session.query(Person).all()\n",
        "for user in all_persons:\n",
        "    print(user.name, user.age)"
      ],
      "metadata": {
        "id": "FIJp7kqS74cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando df podemos hacer la misma consulta\n",
        "df = pd.read_sql('select * from person where name = \\'John\\' ', engine)\n",
        "df"
      ],
      "metadata": {
        "id": "RJlC4kdW8SgR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}